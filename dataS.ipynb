{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 順番に実行し、多値クラス多値ラベル問題を解決できることを確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700 total articles\n",
      "7713 training data\n",
      "2987 testing data\n",
      "55  categories\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'cocoa', 'coffee', 'copper', 'corn', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'hog', 'housing', 'interest', 'ipi', 'iron-steel', 'jobs', 'lead', 'livestock', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'oilseed', 'orange', 'palm-oil', 'pet-chem', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "\n",
    "# Loading the corpus\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# Load MA_Reuters\n",
    "documents = ma_reuters.fileids()\n",
    "print (str(len(documents)) + \" total articles\")\n",
    "# extracting training and testing data (document ID)\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "print (str(len(train_docs_id)) + \" training data\")\n",
    "print (str(len(test_docs_id)) + \" testing data\")\n",
    "# Training and testing data\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    " \n",
    "# print the total number of categories\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "print (num_categories, \" categories\")\n",
    "print (categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANADA PLANS TO MONITOR STEEL IMPORTS, EXPORTS, TRADE MINISTER SAYS\n",
      "\n",
      "  CANADA PLANS TO MONITOR STEEL IMPORTS, EXPORTS, TRADE MINISTER SAYS\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw document example（'coffee category')\n",
    "# Documents in a category\n",
    "category_docs = ma_reuters.fileids(\"iron-steel\");\n",
    "document_id = category_docs[0] # The first document\n",
    "# print the inside document\n",
    "print (ma_reuters.raw(document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to TF-IF model\n",
      "training document dimension ： (7713, 26979)\n",
      "testing document dimension： (2987, 26979)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import re # regular expression\n",
    " \n",
    "def tokenize(text): # returning tokens\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "    return filtered_tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "# fit_transform\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "# transform\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "print(\"converted to TF-IF model\")\n",
    "print(\"training document dimension ：\",vectorised_train_documents.shape)\n",
    "print(\"testing document dimension：\",vectorised_test_documents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.86\n",
      "Hamming Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "import numpy as np\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Jaccard 係数（資料第6回参照）が最も高いカテゴリーと最も低いカテゴリーが何であったかのべよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max categories: ['earn']\n",
      "min categories: ['lead', 'pet-chem', 'soy-oil', 'strategic-metal', 'yen']\n"
     ]
    }
   ],
   "source": [
    "j_scores = jaccard_score(test_labels, OVR_predictions, average=None)\n",
    "j_maxs = [categories[i] for i, v in enumerate(j_scores) if v == max(j_scores)]\n",
    "print(\"max categories:\",j_maxs)\n",
    "j_mins = [categories[i] for i, v in enumerate(j_scores) if v == min(j_scores)]\n",
    "print(\"min categories:\",j_mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 TF-IDFモデル＋SVM以外の組み合わせで本マルチラベル問題に対応できる手法を適宜ためし、実行結果をのべよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器をパーセプトロンにして実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.856\n",
      "Hamming Loss: 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(Perceptron(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器をロジスティック回帰にして実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.709\n",
      "Hamming Loss: 0.009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(LogisticRegression(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類器を多層パーセプトロンにして実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.85\n",
      "Hamming Loss: 0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aobat\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "MLP_Classifier = MLPClassifier(random_state=41, max_iter=300)\n",
    "MLP_Classifier.fit(vectorised_train_documents, train_labels)\n",
    "MLP_predictions = MLP_Classifier.predict(vectorised_test_documents)\n",
    "\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, MLP_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, MLP_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時間がかかったわりに性能が上がらなかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf からOkapi BM25にモデルを変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BSD 3-Clause License\n",
    "\n",
    "#Copyright (c) 2018, Sho IIZUKA\n",
    "#All rights reserved.\n",
    "\n",
    "#Redistribution and use in source and binary forms, with or without\n",
    "#modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "#* Redistributions of source code must retain the above copyright notice, this\n",
    "#  list of conditions and the following disclaimer.\n",
    "\n",
    "#* Redistributions in binary form must reproduce the above copyright notice,\n",
    "#  this list of conditions and the following disclaimer in the documentation\n",
    "#  and/or other materials provided with the distribution.\n",
    "\n",
    "#* Neither the name of the copyright holder nor the names of its\n",
    "#  contributors may be used to endorse or promote products derived from\n",
    "#  this software without specific prior written permission.\n",
    "\n",
    "#THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "#AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "#IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "#DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "#FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "#DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "#SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "#CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "#OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "#OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.feature_extraction.text import _document_frequency\n",
    "\n",
    "class BM25Transformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_idf : boolean, optional (default=True)\n",
    "    k1 : float, optional (default=2.0)\n",
    "    b : float, optional (default=0.75)\n",
    "    References\n",
    "    ----------\n",
    "    Okapi BM25: a non-binary model - Introduction to Information Retrieval\n",
    "    http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html\n",
    "    \"\"\"\n",
    "    def __init__(self, use_idf=True, k1=2.0, b=0.75):\n",
    "        self.use_idf = use_idf\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            document-term matrix\n",
    "        \"\"\"\n",
    "        if not sp.issparse(X):\n",
    "            X = sp.csc_matrix(X)\n",
    "        if self.use_idf:\n",
    "            n_samples, n_features = X.shape\n",
    "            df = _document_frequency(X)\n",
    "            idf = np.log((n_samples - df + 0.5) / (df + 0.5))\n",
    "            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, n=n_features)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, copy=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            document-term matrix\n",
    "        copy : boolean, optional (default=True)\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
    "            # preserve float family dtype\n",
    "            X = sp.csr_matrix(X, copy=copy)\n",
    "        else:\n",
    "            # convert counts or binary occurrences to floats\n",
    "            X = sp.csr_matrix(X, dtype=np.float64, copy=copy)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Document length (number of terms) in each row\n",
    "        # Shape is (n_samples, 1)\n",
    "        dl = X.sum(axis=1)\n",
    "        # Number of non-zero elements in each row\n",
    "        # Shape is (n_samples, )\n",
    "        sz = X.indptr[1:] - X.indptr[0:-1]\n",
    "        # In each row, repeat `dl` for `sz` times\n",
    "        # Shape is (sum(sz), )\n",
    "        # Example\n",
    "        # -------\n",
    "        # dl = [4, 5, 6]\n",
    "        # sz = [1, 2, 3]\n",
    "        # rep = [4, 5, 5, 6, 6, 6]\n",
    "        rep = np.repeat(np.asarray(dl), sz)\n",
    "        # Average document length\n",
    "        # Scalar value\n",
    "        avgdl = np.average(dl)\n",
    "        # Compute BM25 score only for non-zero elements\n",
    "        data = X.data * (self.k1 + 1) / (X.data + self.k1 * (1 - self.b + self.b * rep / avgdl))\n",
    "        X = sp.csr_matrix((data, X.indices, X.indptr), shape=X.shape)\n",
    "\n",
    "        if self.use_idf:\n",
    "            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n",
    "\n",
    "            expected_n_features = self._idf_diag.shape[0]\n",
    "            if n_features != expected_n_features:\n",
    "                raise ValueError(\"Input has n_features=%d while the model\"\n",
    "                                 \" has been trained with n_features=%d\" % (\n",
    "                                     n_features, expected_n_features))\n",
    "            # *= doesn't work\n",
    "            X = X * self._idf_diag\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "class BM25Vectorizer(CountVectorizer):\n",
    "    def __init__(self, *, input='content', encoding='utf-8',\n",
    "             decode_error='strict', strip_accents=None, lowercase=True,\n",
    "             preprocessor=None, tokenizer=None, analyzer='word',\n",
    "             stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "             ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "             max_features=None, vocabulary=None, binary=False,\n",
    "             dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "             sublinear_tf=False):\n",
    "\n",
    "        super().__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self._tfidf = BM25Transformer(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to Okapi BM25 model\n",
      "training document dimension ： (7713, 26979)\n",
      "testing document dimension： (2987, 26979)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text): # returning tokens\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "    return filtered_tokens\n",
    "\n",
    "# Okapi BM25 vectorizer\n",
    "vectorizer = BM25Vectorizer(stop_words='english', tokenizer=tokenize)\n",
    "# fit_transform\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "# transform\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "print(\"converted to Okapi BM25 model\")\n",
    "print(\"training document dimension ：\",vectorised_train_documents.shape)\n",
    "print(\"testing document dimension：\",vectorised_test_documents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.856\n",
      "Hamming Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41,max_iter=100000)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "import numpy as np\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.831\n",
      "Hamming Loss: 0.008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(Perceptron(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfIdf からOkapi BM25にモデルを変更したが、max_iterを増やさないといけなかったりと性能が落ちた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
