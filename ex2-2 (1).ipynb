{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the subset of Reuters news by NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700 total articles\n",
      "7713 training data\n",
      "2987 testing data\n",
      "55  categories\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'cocoa', 'coffee', 'copper', 'corn', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'hog', 'housing', 'interest', 'ipi', 'iron-steel', 'jobs', 'lead', 'livestock', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'oilseed', 'orange', 'palm-oil', 'pet-chem', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "\n",
    "# Loading the corpus\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# Load MA_Reuters\n",
    "documents = ma_reuters.fileids()\n",
    "print (str(len(documents)) + \" total articles\")\n",
    "# extracting training and testing data (document ID)\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "print (str(len(train_docs_id)) + \" training data\")\n",
    "print (str(len(test_docs_id)) + \" testing data\")\n",
    "# Training and testing data\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    " \n",
    "# print the total number of categories\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "print (num_categories, \" categories\")\n",
    "print (categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Sample Code Check\n",
    "## Coffee category data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUNDESBANK ALLOCATES 6.1 BILLION MARKS IN TENDER\n",
      "  The Bundesbank accepted bids for 6.1\n",
      "  billion marks at today's tender for a 28-day securities\n",
      "  repurchase pact at a fixed rate of 3.80 pct, a central bank\n",
      "  spokesman said.\n",
      "      Banks, which bid for a total 12.2 billion marks liquidity,\n",
      "  will be credited with the funds allocated today and must buy\n",
      "  back securities pledged on May 6.\n",
      "      Some 14.9 billion marks will drain from the market today as\n",
      "  an earlier pact expires, so the Bundesbank is effectively\n",
      "  withdrawing a net 8.1 billion marks from the market with\n",
      "  today's allocation.\n",
      "      A Bundesbank spokesman said in answer to enquiries that the\n",
      "  withdrawal of funds did not reflect a tightening of credit\n",
      "  policy, but was to be seen in the context of plentiful\n",
      "  liquidity in the banking system.\n",
      "      Banks held an average 59.3 billion marks at the Bundesbank\n",
      "  over the first six days of the month, well clear of the likely\n",
      "  April minimum reserve requirement of 51 billion marks.\n",
      "      The Bundesbank spokesman noted that by bidding only 12.2\n",
      "  billion marks, below the outgoing 14.9 billion, banks\n",
      "  themselves had shown they felt they had plenty of liquidity.\n",
      "      Dealers said the Bundesbank is keen to prevent too much\n",
      "  liquidity accruing in the market, as that would blunt the\n",
      "  effectiveness of the security repurchase agreement, its main\n",
      "  open-market instrument for steering market interest rates. Two\n",
      "  further pacts are likely this month over the next two weeks.\n",
      "      The Bundesbank is currently steering call money between 3.6\n",
      "  and 3.8 pct, although short-term fluctuations outside that\n",
      "  range are possible, dealers said.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw document example（'coffee category')\n",
    "# Documents in a category\n",
    "category_docs = ma_reuters.fileids(\"money-fx\");\n",
    "document_id = category_docs[0] # The first document\n",
    "# print the inside document\n",
    "print (ma_reuters.raw(document_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with NLTK, TF-IDF vectorizer with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to TF-IF model\n",
      "training document dimension ： (7713, 26985)\n",
      "testing document dimension： (2987, 26985)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import re # regular expression\n",
    " \n",
    "def tokenize(text): # returning tokens\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "    return filtered_tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "# fit_transform\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "# transform\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "print(\"converted to TF-IF model\")\n",
    "print(\"training document dimension ：\",vectorised_train_documents.shape)\n",
    "print(\"testing document dimension：\",vectorised_test_documents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.86\n",
      "Hamming Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "import numpy as np\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Jaccard Index; What category is the highest, and the lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 'lead'), (0.0, 'pet-chem'), (0.0, 'soy-oil'), (0.0, 'strategic-metal'), (0.0, 'yen'), (0.125, 'silver'), (0.2, 'fuel'), (0.231, 'soy-meal'), (0.273, 'sorghum'), (0.316, 'meal-feed'), (0.333, 'hog'), (0.36, 'rice'), (0.407, 'livestock'), (0.414, 'cpi'), (0.421, 'carcass'), (0.421, 'veg-oil'), (0.435, 'alum'), (0.436, 'soybean'), (0.444, 'rapeseed'), (0.462, 'zinc'), (0.471, 'gas'), (0.471, 'nat-gas'), (0.5, 'oilseed'), (0.5, 'retail'), (0.5, 'wpi'), (0.524, 'jobs'), (0.55, 'cotton'), (0.556, 'iron-steel'), (0.579, 'reserves'), (0.6, 'palm-oil'), (0.621, 'interest'), (0.625, 'bop'), (0.625, 'gold'), (0.643, 'barley'), (0.651, 'money-fx'), (0.653, 'dlr'), (0.66, 'ship'), (0.667, 'tin'), (0.672, 'trade'), (0.697, 'wheat'), (0.703, 'money-supply'), (0.724, 'corn'), (0.727, 'orange'), (0.75, 'gnp'), (0.75, 'housing'), (0.75, 'rubber'), (0.757, 'sugar'), (0.81, 'crude'), (0.833, 'copper'), (0.833, 'ipi'), (0.836, 'grain'), (0.931, 'coffee'), (0.94, 'acq'), (0.944, 'cocoa'), (0.969, 'earn')]\n"
     ]
    }
   ],
   "source": [
    "jaccard_indices = np.round(jaccard_score(test_labels, OVR_predictions, average=None),3) # Code 1\n",
    "jaccard_indices_with_category_name = list(zip(jaccard_indices, categories)) # Code 2\n",
    "jaccard_indices_with_category_name.sort()\n",
    "\n",
    "print(jaccard_indices_with_category_name) # Code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if 'categories' and jaccard_indices is same order\n",
    "import itertools\n",
    "[list(itertools.compress(categories, [True if j==1 else False for j in i])) for i in test_labels] == [ma_reuters.categories(doc_id) for doc_id in test_docs_id] # Code 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code 1で算出したJaccard係数と，\"Loading the subset of Reuters news by NLTK\"で準備してある変数categoriesの順序が一致するか，Code 4で調べた．\n",
    "Code 4では，全てのテストデータについて，バイナライズされた多値ラベル表現と実際のラベルの順序関係が一致しているかを調べている．\n",
    "Jaccard係数の算出では，バイナライズされた多値ラベル表現を用いているため，出力はこの表現と同順である．\n",
    "Code 4の出力がTrueであることから，Code 1のjaccard_indicesとcategoriesは同順序で保持されていることを確認した．\n",
    "\n",
    "Code 2でjaccard_indicesとcategoriesを結合したリストを生成し，これをJaccard係数でソートすることにより，Code 3の出力を得た．\n",
    "この結果，最もJaccard係数の高いカテゴリーは”earn”であり，最も低いカテゴリーは”lead”，”pet-chem”，”soy-oil”，”strategic-metal”，”yen”（全てJaccard係数が0）であった．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Other method for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "def getDocumentVectors(docs, model):\n",
    "    vectors = np.zeros((len(docs),EMBED_SIZE))\n",
    "    for i, text in enumerate(docs):\n",
    "        tokenized_text = tokenize(text)\n",
    "        for word in tokenized_text:\n",
    "            try:\n",
    "                vectors[i, :] = np.add(vectors[i, :], model[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(tokenized_text) > 0:\n",
    "            vectors[i, :] = np.divide(vectors[i, :], len(tokenized_text))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "wordvectors_google = KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to the mean of Word2Vec model\n",
      "training document dimension ： (7713, 300)\n",
      "testing document dimension： (2987, 300)\n"
     ]
    }
   ],
   "source": [
    "# fit_transform\n",
    "vectorised_train_documents_google = getDocumentVectors(train_docs,wordvectors_google)\n",
    "# transform\n",
    "vectorised_test_documents_google = getDocumentVectors(test_docs,wordvectors_google)\n",
    "print(\"converted to the mean of Word2Vec model\")\n",
    "print(\"training document dimension ：\",vectorised_train_documents_google.shape)\n",
    "print(\"testing document dimension：\",vectorised_test_documents_google.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.773\n",
      "Hamming Loss: 0.007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier_google = OneVsRestClassifier(SVC()) \n",
    "OVR_classifier_google.fit(vectorised_train_documents_google, train_labels)\n",
    "OVR_predictions_google = OVR_classifier_google.predict(vectorised_test_documents_google)\n",
    "\n",
    "import numpy as np\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions_google, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions_google),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 'alum'), (0.0, 'barley'), (0.0, 'carcass'), (0.0, 'fuel'), (0.0, 'gnp'), (0.0, 'iron-steel'), (0.0, 'lead'), (0.0, 'meal-feed'), (0.0, 'orange'), (0.0, 'pet-chem'), (0.0, 'retail'), (0.0, 'rice'), (0.0, 'rubber'), (0.0, 'silver'), (0.0, 'sorghum'), (0.0, 'soy-meal'), (0.0, 'soy-oil'), (0.0, 'strategic-metal'), (0.0, 'tin'), (0.0, 'wpi'), (0.0, 'yen'), (0.0, 'zinc'), (0.033, 'nat-gas'), (0.05, 'cotton'), (0.091, 'soybean'), (0.1, 'palm-oil'), (0.104, 'oilseed'), (0.108, 'veg-oil'), (0.111, 'copper'), (0.167, 'bop'), (0.167, 'ipi'), (0.176, 'gas'), (0.222, 'cocoa'), (0.25, 'housing'), (0.269, 'livestock'), (0.31, 'cpi'), (0.333, 'hog'), (0.333, 'reserves'), (0.371, 'money-supply'), (0.375, 'corn'), (0.444, 'rapeseed'), (0.484, 'gold'), (0.489, 'dlr'), (0.5, 'sugar'), (0.524, 'jobs'), (0.544, 'interest'), (0.571, 'coffee'), (0.588, 'money-fx'), (0.622, 'wheat'), (0.628, 'ship'), (0.631, 'trade'), (0.687, 'grain'), (0.768, 'crude'), (0.907, 'acq'), (0.964, 'earn')]\n"
     ]
    }
   ],
   "source": [
    "jaccard_indices_google = np.round(jaccard_score(test_labels, OVR_predictions_google, average=None),3) # Code 1\n",
    "jaccard_indices_with_category_name_google = list(zip(jaccard_indices_google, categories)) # Code 2\n",
    "jaccard_indices_with_category_name_google.sort()\n",
    "\n",
    "print(jaccard_indices_with_category_name_google) # Code 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vecを用いて各ドキュメントに含まれる単語の分散表現を取得し，その平均をドキュメントの分散表現とした．\n",
    "分類器は[1]と同様にSVMを用いた．したがって，[1]の結果と[3]の結果を比較することができる．\n",
    "\n",
    "Word2Vecを用いた文書分類は，TF-IDFを用いた場合と比べ，性能が低下した．\n",
    "特に，カテゴリ別のJaccard係数を比べると，Word2Vecを用いた場合に0.0となっているカテゴリが増えている．\n",
    "Word2Vecは未知語に対して分散表現を与えることができない．\n",
    "そのため，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
