{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニュース記事からの単語の抽出とカウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700 total articles\n",
      "7713 training data\n",
      "2987 testing data\n",
      "55  categories\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'cocoa', 'coffee', 'copper', 'corn', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'hog', 'housing', 'interest', 'ipi', 'iron-steel', 'jobs', 'lead', 'livestock', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'oilseed', 'orange', 'palm-oil', 'pet-chem', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "\n",
    "# Loading the corpus\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# Load MA_Reuters\n",
    "documents = ma_reuters.fileids()\n",
    "print (str(len(documents)) + \" total articles\")\n",
    "# extracting training and testing data (document ID)\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "print (str(len(train_docs_id)) + \" training data\")\n",
    "print (str(len(test_docs_id)) + \" testing data\")\n",
    "# Training and testing data\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    " \n",
    "# print the total number of categories\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "print (num_categories, \" categories\")\n",
    "print (categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3  \n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "\n",
    "# Kerasの定義\n",
    "import keras\n",
    "#from keras import backend as K\n",
    "#import keras.backend.tensorflow_backend as K\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "K.set_session(session) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen =  1094\n",
      " Word count =  32662   <class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "# ニュース記事に現れる単語→インデックス辞書の作成\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import collections\n",
    "import re\n",
    " \n",
    "maxlen = 20 # 1文書に含まれる層単語数の上限を保持\n",
    "min_length = 3 # 1単語の文字数の最小値(3文字以上の単語のみ残す)\n",
    "word_counter = collections.Counter()\n",
    "docs = [train_docs, test_docs]\n",
    "\n",
    "for document in docs: # 単語の小文字化と抽出\n",
    "    num_data = len(document)\n",
    "    for i in range(num_data):\n",
    "        text = document[i]\n",
    "        words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "        p = re.compile('[a-zA-Z]+')\n",
    "        filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "        if len(filtered_tokens) > maxlen:\n",
    "            maxlen = len(filtered_tokens)\n",
    "        for word in filtered_tokens:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "print(\"maxlen = \",maxlen)\n",
    "print(\" Word count = \", len(word_counter),' ',type(word_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙生成 creating vocabulary...\n",
      "len(word2index) =  25000\n",
      "index2word[1] =  the\n"
     ]
    }
   ],
   "source": [
    "print(\"語彙生成 creating vocabulary...\")\n",
    "VOCAB_SIZE = 25000 # Reuters News 最大語彙の設定（これ以上は無視する）\n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(word_counter.most_common(VOCAB_SIZE)):# 頻度順\n",
    "    word2index[word[0]] = wid + 1\n",
    "vocab_sz = len(word2index) + 1\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[0] = \"_UNK_\" # 未知語\n",
    "print(\"len(word2index) = \", len(word2index))\n",
    "print(\"index2word[1] = \",index2word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#順引き辞書と逆引き辞書の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙生成 creating vocabulary...\n",
      "len(word2index) =  27000\n",
      "index2word[1] =  the\n"
     ]
    }
   ],
   "source": [
    "print(\"語彙生成 creating vocabulary...\")\n",
    "VOCAB_SIZE = 27000 # Reuters News 最大語彙の設定（これ以上は無視する）\n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(word_counter.most_common(VOCAB_SIZE)):# 頻度順\n",
    "    word2index[word[0]] = wid + 1\n",
    "vocab_sz = len(word2index) + 1\n",
    "# 逆引き辞書作成\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[0] = \"_UNK_\" # 未知語\n",
    "print(\"len(word2index) = \", len(word2index))\n",
    "print(\"index2word[1] = \",index2word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) for doc_id in test_docs_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練用データの単語列生成 creating word sequences...\n",
      "訓練データ（データ＋ラベル）\n",
      "X_train int32   <class 'numpy.ndarray'>   (7713, 1094)\n",
      "Y_train int64   <class 'numpy.ndarray'>   (7713, 55)\n",
      "テスト用データの単語列生成 creating word sequences...\n",
      "テストデータ（データ＋ラベル）\n",
      "X_test int32   <class 'numpy.ndarray'>   (2987, 1094)\n",
      "Y_test int64   <class 'numpy.ndarray'>   (2987, 55)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"訓練用データの単語列生成 creating word sequences...\")\n",
    "\n",
    "min_length = 3\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "xs_train = []\n",
    "document = train_docs\n",
    "num_data = len(document)\n",
    "for i in range(num_data):\n",
    "    text = document[i]\n",
    "    words = [x.lower() for x in word_tokenize(text)] # NLTK's word tokenizer\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "    wids = [word2index[word] for word in filtered_tokens]\n",
    "    xs_train.append(wids)\n",
    "            \n",
    "X_train = pad_sequences(xs_train, maxlen=maxlen)# パディング (1861単語が最大)\n",
    "Y_train = train_labels # np_utils.to_categorical(ys)  多値分類なのでワンホットではない!!\n",
    "print(\"訓練データ（データ＋ラベル）\")\n",
    "print(\"X_train\",X_train.dtype,\" \",type(X_train),\" \",X_train.shape)\n",
    "print(\"Y_train\",Y_train.dtype,\" \",type(Y_train),\" \",Y_train.shape)\n",
    "\n",
    "print(\"テスト用データの単語列生成 creating word sequences...\")\n",
    "xs_test = []\n",
    "document = test_docs\n",
    "num_data = len(document)\n",
    "for i in range(num_data):\n",
    "    text = document[i]\n",
    "    words = [x.lower() for x in word_tokenize(text)] # NLTK's word tokenizer\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "    wids = [word2index[word] for word in filtered_tokens]\n",
    "    # wids = [word2index[word] for word in words]\n",
    "    xs_test.append(wids)\n",
    "\n",
    "X_test = pad_sequences(xs_test, maxlen=maxlen)# パディング\n",
    "Y_test = test_labels # np_utils.to_categorical(ys) 多値分類なのでワンホットではない!!\n",
    "print(\"テストデータ（データ＋ラベル）\")\n",
    "print(\"X_test\",X_test.dtype,\" \",type(X_test),\" \",X_test.shape)\n",
    "print(\"Y_test\",Y_test.dtype,\" \",type(Y_test),\" \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7713, 1094) (2987, 1094) (7713, 55) (2987, 55)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = X_train\n",
    "Xtest = X_test\n",
    "Ytrain = Y_train\n",
    "Ytest = Y_test\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "\n",
    "# 分散表現モデル\n",
    "WORD2VEC_MODEL = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# 最大語彙サイズ\n",
    "VOCAB_SIZE = 27000 \n",
    "\n",
    "# Google Newsで学習された300次元のword embedding(分散表現)\n",
    "EMBED_SIZE = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not a gzipped file (b've')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f536eef0304c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORD2VEC_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1547\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\037\\213'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not a gzipped file (%r)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         (method, flag,\n",
      "\u001b[0;31mOSError\u001b[0m: Not a gzipped file (b've')"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True)\n",
    "embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))\n",
    "for word, index in word2index.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(\"Embedding_weight matrix size = \", embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNNの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1096)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1096, 300)         8100300   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 1096, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1092, 256)         384256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 55)                28215     \n",
      "=================================================================\n",
      "Total params: 8,644,355\n",
      "Trainable params: 8,644,355\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "NUM_CLASSES = 55 \n",
    "NUM_FILTERS = 256 \n",
    "NUM_WORDS = 5 \n",
    "\n",
    "inputs = Input(shape=(maxlen,)) \n",
    "x = Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,\n",
    "                    weights=[embedding_weights], # 初期値をGoogleの分散表現にする\n",
    "                    trainable=True)(inputs)\n",
    "x = SpatialDropout1D(0.3)(x) \n",
    "x = Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"elu\")(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(512,activation=\"elu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation=\"sigmoid\")(x)\n",
    "model3 = Model(inputs=[inputs], outputs=[outputs])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "61/61 [==============================] - 169s 3s/step - loss: 0.1091 - categorical_accuracy: 0.4349 - val_loss: 0.0524 - val_categorical_accuracy: 0.6304\n",
      "Epoch 2/40\n",
      "61/61 [==============================] - 161s 3s/step - loss: 0.0481 - categorical_accuracy: 0.6572 - val_loss: 0.0363 - val_categorical_accuracy: 0.7516\n",
      "Epoch 3/40\n",
      "61/61 [==============================] - 160s 3s/step - loss: 0.0352 - categorical_accuracy: 0.7402 - val_loss: 0.0305 - val_categorical_accuracy: 0.7740\n",
      "Epoch 4/40\n",
      "61/61 [==============================] - 160s 3s/step - loss: 0.0276 - categorical_accuracy: 0.7836 - val_loss: 0.0255 - val_categorical_accuracy: 0.7975\n",
      "Epoch 5/40\n",
      "61/61 [==============================] - 158s 3s/step - loss: 0.0216 - categorical_accuracy: 0.8172 - val_loss: 0.0222 - val_categorical_accuracy: 0.8219\n",
      "Epoch 6/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0173 - categorical_accuracy: 0.8398 - val_loss: 0.0200 - val_categorical_accuracy: 0.8283\n",
      "Epoch 7/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0138 - categorical_accuracy: 0.8608 - val_loss: 0.0183 - val_categorical_accuracy: 0.8353\n",
      "Epoch 8/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0113 - categorical_accuracy: 0.8737 - val_loss: 0.0173 - val_categorical_accuracy: 0.8480\n",
      "Epoch 9/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0093 - categorical_accuracy: 0.8818 - val_loss: 0.0168 - val_categorical_accuracy: 0.8514\n",
      "Epoch 10/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0078 - categorical_accuracy: 0.8867 - val_loss: 0.0174 - val_categorical_accuracy: 0.8483\n",
      "Epoch 11/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0067 - categorical_accuracy: 0.8955 - val_loss: 0.0171 - val_categorical_accuracy: 0.8520\n",
      "Epoch 12/40\n",
      "61/61 [==============================] - 160s 3s/step - loss: 0.0056 - categorical_accuracy: 0.8996 - val_loss: 0.0166 - val_categorical_accuracy: 0.8607\n",
      "Epoch 13/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0045 - categorical_accuracy: 0.8994 - val_loss: 0.0164 - val_categorical_accuracy: 0.8634\n",
      "Epoch 14/40\n",
      "61/61 [==============================] - 157s 3s/step - loss: 0.0040 - categorical_accuracy: 0.9054 - val_loss: 0.0163 - val_categorical_accuracy: 0.8654\n",
      "Epoch 15/40\n",
      "61/61 [==============================] - 160s 3s/step - loss: 0.0037 - categorical_accuracy: 0.9047 - val_loss: 0.0172 - val_categorical_accuracy: 0.8637\n",
      "Epoch 16/40\n",
      "61/61 [==============================] - 160s 3s/step - loss: 0.0034 - categorical_accuracy: 0.9056 - val_loss: 0.0171 - val_categorical_accuracy: 0.8664\n",
      "Epoch 17/40\n",
      "61/61 [==============================] - 159s 3s/step - loss: 0.0029 - categorical_accuracy: 0.9086 - val_loss: 0.0173 - val_categorical_accuracy: 0.8647\n",
      "Epoch 18/40\n",
      "61/61 [==============================] - 158s 3s/step - loss: 0.0028 - categorical_accuracy: 0.9081 - val_loss: 0.0173 - val_categorical_accuracy: 0.8658\n",
      "Epoch 19/40\n",
      "61/61 [==============================] - 162s 3s/step - loss: 0.0025 - categorical_accuracy: 0.9081 - val_loss: 0.0182 - val_categorical_accuracy: 0.8668\n",
      "Epoch 20/40\n",
      "61/61 [==============================] - 158s 3s/step - loss: 0.0025 - categorical_accuracy: 0.9033 - val_loss: 0.0174 - val_categorical_accuracy: 0.8708\n",
      "Epoch 21/40\n",
      "61/61 [==============================] - 162s 3s/step - loss: 0.0022 - categorical_accuracy: 0.9095 - val_loss: 0.0182 - val_categorical_accuracy: 0.8684\n",
      "Epoch 22/40\n",
      "61/61 [==============================] - 205s 3s/step - loss: 0.0022 - categorical_accuracy: 0.9060 - val_loss: 0.0186 - val_categorical_accuracy: 0.8708\n",
      "Epoch 23/40\n",
      "61/61 [==============================] - 181s 3s/step - loss: 0.0021 - categorical_accuracy: 0.9091 - val_loss: 0.0182 - val_categorical_accuracy: 0.8671\n",
      "Epoch 24/40\n",
      "61/61 [==============================] - 200s 3s/step - loss: 0.0019 - categorical_accuracy: 0.9099 - val_loss: 0.0177 - val_categorical_accuracy: 0.8684\n",
      "Epoch 25/40\n",
      "61/61 [==============================] - 252s 4s/step - loss: 0.0017 - categorical_accuracy: 0.9082 - val_loss: 0.0192 - val_categorical_accuracy: 0.8731\n",
      "Epoch 26/40\n",
      "61/61 [==============================] - 253s 4s/step - loss: 0.0017 - categorical_accuracy: 0.9048 - val_loss: 0.0193 - val_categorical_accuracy: 0.8721\n",
      "Epoch 27/40\n",
      "61/61 [==============================] - 255s 4s/step - loss: 0.0017 - categorical_accuracy: 0.9051 - val_loss: 0.0194 - val_categorical_accuracy: 0.8714\n",
      "Epoch 28/40\n",
      "61/61 [==============================] - 222s 4s/step - loss: 0.0018 - categorical_accuracy: 0.9078 - val_loss: 0.0198 - val_categorical_accuracy: 0.8658\n",
      "Epoch 29/40\n",
      "61/61 [==============================] - 191s 3s/step - loss: 0.0016 - categorical_accuracy: 0.9035 - val_loss: 0.0195 - val_categorical_accuracy: 0.8721\n",
      "Epoch 30/40\n",
      "61/61 [==============================] - 188s 3s/step - loss: 0.0016 - categorical_accuracy: 0.9111 - val_loss: 0.0207 - val_categorical_accuracy: 0.8698\n",
      "Epoch 31/40\n",
      "61/61 [==============================] - 186s 3s/step - loss: 0.0015 - categorical_accuracy: 0.9099 - val_loss: 0.0207 - val_categorical_accuracy: 0.8661\n",
      "Epoch 32/40\n",
      "61/61 [==============================] - 189s 3s/step - loss: 0.0015 - categorical_accuracy: 0.9038 - val_loss: 0.0202 - val_categorical_accuracy: 0.8701\n",
      "Epoch 33/40\n",
      "61/61 [==============================] - 175s 3s/step - loss: 0.0016 - categorical_accuracy: 0.9061 - val_loss: 0.0206 - val_categorical_accuracy: 0.8728\n",
      "Epoch 34/40\n",
      "61/61 [==============================] - 184s 3s/step - loss: 0.0015 - categorical_accuracy: 0.9072 - val_loss: 0.0208 - val_categorical_accuracy: 0.8701\n",
      "Epoch 35/40\n",
      "61/61 [==============================] - 173s 3s/step - loss: 0.0013 - categorical_accuracy: 0.9050 - val_loss: 0.0196 - val_categorical_accuracy: 0.8721\n",
      "Epoch 36/40\n",
      "61/61 [==============================] - 187s 3s/step - loss: 0.0014 - categorical_accuracy: 0.9051 - val_loss: 0.0227 - val_categorical_accuracy: 0.8738\n",
      "Epoch 37/40\n",
      "61/61 [==============================] - 191s 3s/step - loss: 0.0014 - categorical_accuracy: 0.9041 - val_loss: 0.0207 - val_categorical_accuracy: 0.8637\n",
      "Epoch 38/40\n",
      "61/61 [==============================] - 188s 3s/step - loss: 0.0015 - categorical_accuracy: 0.9079 - val_loss: 0.0213 - val_categorical_accuracy: 0.8668\n",
      "Epoch 39/40\n",
      "61/61 [==============================] - 189s 3s/step - loss: 0.0014 - categorical_accuracy: 0.9037 - val_loss: 0.0212 - val_categorical_accuracy: 0.8661\n",
      "Epoch 40/40\n",
      "61/61 [==============================] - 192s 3s/step - loss: 0.0013 - categorical_accuracy: 0.9078 - val_loss: 0.0205 - val_categorical_accuracy: 0.8668\n"
     ]
    }
   ],
   "source": [
    "model3.compile(\n",
    "    optimizer=\"adam\",  \n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 128 \n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "fpath = 'h5/Reuters-CNN-w-{epoch:02d}-{loss:.4f}-{val_loss:.4f}.h5'\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(fpath, monitor='val_loss', save_best_only=True),\n",
    "]\n",
    "\n",
    "history3 = model3.fit(\n",
    "    Xtrain, Ytrain, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(Xtest, Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3165"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model1 and history1\n",
    "model3.save_weights('/Users/kosukehama/Python_DataScience/MA_Reuters-CNN-2020-8-4-weights.h5')\n",
    "model3.save('/Users/kosukehama/Python_DataScience/MA_Reuters-CNN-2020-8-4.h5')\n",
    "\n",
    "import pickle\n",
    "with open('/Users/kosukehama/Python_DataScience/MA_Reuters-CNN-2020-8-4.pkl', 'wb') as h_file:\n",
    "    pickle.dump(history3.history, h_file)\n",
    "\n",
    "json_str = model3.to_json()\n",
    "open('/Users/kosukehama/Python_DataScience/MA_Reuters-CNN-2020-8-4.json','w').write(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model from disk\n",
      "\n",
      "\n",
      "テストデータの損失: 0.0205 (カテゴリカル精度: 0.867) \n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('/Users/kosukehama/Python_DataScience/MA_Reuters-CNN-2020-8-4.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/Users/kosukehama/Python_DataScience/MA_Reuters-CNN-2020-8-4-weights.h5\")\n",
    "print(\"Loaded the best model from disk\")\n",
    " \n",
    "loaded_model.compile(\n",
    "    optimizer=\"adam\", # sgd, # \"adadelta\", # sgd, # 'adadelta', # sgd, # \"adadelta\", \n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "score = loaded_model.score = loaded_model.evaluate(Xtest, Ytest, verbose=0)\n",
    "\n",
    "print()\n",
    "print(\"\\nテストデータの損失: {:.4f} (カテゴリカル精度: {:.3f}) \".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
