{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# データ情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10700 total articles\n",
      "7713 training data\n",
      "2987 testing data\n",
      "55  categories\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'cocoa', 'coffee', 'copper', 'corn', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'hog', 'housing', 'interest', 'ipi', 'iron-steel', 'jobs', 'lead', 'livestock', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'oilseed', 'orange', 'palm-oil', 'pet-chem', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import *\n",
    "\n",
    "# Loading the corpus\n",
    "ma_reuters = LazyCorpusLoader(\n",
    "    'ma_reuters', CategorizedPlaintextCorpusReader, '(training|test).*',\n",
    "    cat_file='cats.txt', encoding='ISO-8859-2')\n",
    "\n",
    "# Load MA_Reuters\n",
    "documents = ma_reuters.fileids()\n",
    "print (str(len(documents)) + \" total articles\")\n",
    "# extracting training and testing data (document ID)\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "print (str(len(train_docs_id)) + \" training data\")\n",
    "print (str(len(test_docs_id)) + \" testing data\")\n",
    "# Training and testing data\n",
    "train_docs = [ma_reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [ma_reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    " \n",
    "# print the total number of categories\n",
    "categories = ma_reuters.categories()\n",
    "num_categories = len(categories)\n",
    "print (num_categories, \" categories\")\n",
    "print (categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ココアカテゴリの文書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCOA EXPORTERS EXPECTED TO LIMIT SALES\n",
      "  Major cocoa exporters are likely to\n",
      "  limit sales in the weeks ahead in an effort to boost world\n",
      "  prices, sources close to a meeting of the Cocoa Producers\n",
      "  Alliance (CPA) said.\n",
      "      The sources said the depressed world market had been one of\n",
      "  the main topics discussed in a closed door meeting of the\n",
      "  11-member CPA which began on Monday.\n",
      "      They said producers agreed that cutting sales would aid the\n",
      "  buffer stock manager of a new international cocoa pact in his\n",
      "  effort to support prices.\n",
      "      Major cocoa producing and consuming nations agreed\n",
      "  operation rules for the buffer stock at a meeting in London\n",
      "  last month and the stock manager is expected to enter the\n",
      "  market soon.\n",
      "      Prices, under the weight of three successive cocoa\n",
      "  surpluses, recently fell to the level at which the manager has\n",
      "  to buy cocoa under stock rules.\n",
      "      The buffer stock aims to keep prices within a pre-set range\n",
      "  by buying when prices fall and selling when they rise.\n",
      "      \"The world's cocoa price at present is just not interesting,\"\n",
      "  commented one delegate representing a major CPA producer.\n",
      "      Another source said that with much of the 1986/87\n",
      "  (October-September) world cocoa crop sold, limiting sales in\n",
      "  the near term concerns essentially next year's harvest.\n",
      "      The sources noted, however, that the cocoa industry in\n",
      "  Brazil, the world's number two producer, is in private hands.\n",
      "  This means limiting sales is more difficult than in major West\n",
      "  African producers, where sales are made or authorized by\n",
      "  commodity marketing boards.\n",
      "      The CPA includes the world's top three producers, Ivory\n",
      "  Coast, Brazil and Ghana, and accounts for 80 pct of all output.\n",
      "      The meeting here is due to end tomorrow evening.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Documents in a category\n",
    "category_docs = ma_reuters.fileids(\"cocoa\");\n",
    "document_id = category_docs[0] # The first document\n",
    "# print the inside document\n",
    "print (ma_reuters.raw(document_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted to TF-IF model\n",
      "training document dimension ： (7713, 26985)\n",
      "testing document dimension： (2987, 26985)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import re # regular expression\n",
    " \n",
    "def tokenize(text): # returning tokens\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "\n",
    "    p = re.compile('[a-zA-Z]+')\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length, words))\n",
    "    return filtered_tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "# fit_transform\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "# transform\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "print(\"converted to TF-IF model\")\n",
    "print(\"training document dimension ：\",vectorised_train_documents.shape)\n",
    "print(\"testing document dimension：\",vectorised_test_documents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: 0.86\n",
      "Hamming Loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([ma_reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([ma_reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# multi-class, multi-label classification and prediction\n",
    "OVR_classifier = OneVsRestClassifier(LinearSVC(random_state=41)) \n",
    "OVR_classifier.fit(vectorised_train_documents, train_labels)\n",
    "OVR_predictions = OVR_classifier.predict(vectorised_test_documents)\n",
    "\n",
    "import numpy as np\n",
    "# Jaccard coefficient\n",
    "from sklearn.metrics import jaccard_score\n",
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average='samples'),3))\n",
    "\n",
    "# Hamming Loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "print (\"Hamming Loss:\",np.round(hamming_loss(test_labels, OVR_predictions),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard coef: [0.94  0.435 0.643 0.625 0.421 0.944 0.931 0.833 0.724 0.55  0.414 0.81\n",
      " 0.653 0.969 0.2   0.471 0.75  0.625 0.836 0.333 0.75  0.621 0.833 0.556\n",
      " 0.524 0.    0.407 0.316 0.651 0.703 0.471 0.5   0.727 0.6   0.    0.444\n",
      " 0.579 0.5   0.36  0.75  0.66  0.125 0.273 0.231 0.    0.436 0.    0.757\n",
      " 0.667 0.672 0.421 0.697 0.5   0.    0.462]\n",
      "Max:\n",
      "earn\n",
      "Min:\n",
      "lead\n",
      "pet-chem\n",
      "soy-oil\n",
      "strategic-metal\n",
      "yen\n"
     ]
    }
   ],
   "source": [
    "print (\"Jaccard coef:\",np.round(jaccard_score(test_labels, OVR_predictions, average=None),3))\n",
    "print(\"Max:\")\n",
    "print(categories[np.argmax(jaccard_score(test_labels, OVR_predictions, average=None))])\n",
    "print(\"Min:\")\n",
    "score = jaccard_score(test_labels, OVR_predictions, average=None)\n",
    "mini = tmp.min()\n",
    "for i in range(len(tmp)):\n",
    "    if mini==tmp[i]:\n",
    "        print(categories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
